"""
è®­ç»ƒå‰å®Œæ•´æ£€æŸ¥è„šæœ¬
ä½œè€…: Larry3301
åŠŸèƒ½: åœ¨æ¨¡å‹è®­ç»ƒå‰è¿›è¡Œå…¨é¢çš„ç¡¬ä»¶ã€æ•°æ®ã€æ¨¡å‹å’Œé…ç½®æ£€æŸ¥
"""

import torch
import torch.nn as nn
import psutil
import shutil
import numpy as np
import os
import sys
from datetime import datetime

class TrainingPreCheck:
    def __init__(self, model=None, dataloader=None, optimizer=None, criterion=None, input_shape=None):
        self.model = model
        self.dataloader = dataloader
        self.optimizer = optimizer
        self.criterion = criterion
        self.input_shape = input_shape
        self.check_results = {}
        
    def print_header(self, title):
        """æ‰“å°æ ‡é¢˜"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {title}")
        print(f"{'='*60}")
    
    def print_success(self, message):
        """æ‰“å°æˆåŠŸä¿¡æ¯"""
        print(f"âœ… {message}")
    
    def print_warning(self, message):
        """æ‰“å°è­¦å‘Šä¿¡æ¯"""
        print(f"âš ï¸  {message}")
    
    def print_error(self, message):
        """æ‰“å°é”™è¯¯ä¿¡æ¯"""
        print(f"âŒ {message}")
    
    def check_system_environment(self):
        """æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ"""
        self.print_header("ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥")
        
        # Pythonç¯å¢ƒ
        self.print_success(f"Pythonç‰ˆæœ¬: {sys.version}")
        self.print_success(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # å·¥ä½œç›®å½•
        cwd = os.getcwd()
        self.print_success(f"å·¥ä½œç›®å½•: {cwd}")
        
        # æ£€æŸ¥å¿…è¦çš„åŒ…
        required_packages = ['torch', 'numpy', 'psutil']
        for package in required_packages:
            try:
                __import__(package)
                self.print_success(f"{package}: å·²å®‰è£…")
            except ImportError:
                self.print_error(f"{package}: æœªå®‰è£…")
    
    def check_hardware_resources(self):
        """æ£€æŸ¥ç¡¬ä»¶èµ„æº"""
        self.print_header("ç¡¬ä»¶èµ„æºæ£€æŸ¥")
        
        # GPUæ£€æŸ¥
        cuda_available = torch.cuda.is_available()
        self.print_success(f"CUDAå¯ç”¨: {cuda_available}")
        
        if cuda_available:
            gpu_count = torch.cuda.device_count()
            self.print_success(f"GPUæ•°é‡: {gpu_count}")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
                current_device = torch.cuda.current_device()
                status = "å½“å‰è®¾å¤‡" if i == current_device else "å¯ç”¨è®¾å¤‡"
                self.print_success(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB) - {status}")
            
            # CUDAç‰ˆæœ¬
            cuda_version = torch.version.cuda
            if cuda_version:
                self.print_success(f"CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            self.print_warning("æœªæ£€æµ‹åˆ°GPUï¼Œå°†åœ¨CPUä¸Šè®­ç»ƒ")
        
        # CPUå’Œå†…å­˜
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        self.print_success(f"CPUæ ¸å¿ƒæ•°: {cpu_count} (ä½¿ç”¨ç‡: {cpu_percent}%)")
        self.print_success(f"å†…å­˜: {memory.total / 1e9:.1f}GB (ä½¿ç”¨ç‡: {memory.percent}%)")
        
        # ç£ç›˜ç©ºé—´
        total, used, free = shutil.disk_usage(".")
        usage_percent = (used / total) * 100
        self.print_success(f"ç£ç›˜ç©ºé—´: {free // (2**30)}GB å¯ç”¨ (æ€»å…±: {total // (2**30)}GB, ä½¿ç”¨ç‡: {usage_percent:.1f}%)")
        
        if usage_percent > 90:
            self.print_warning("ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå»ºè®®æ¸…ç†ç©ºé—´")
    
    def check_data_pipeline(self):
        """æ£€æŸ¥æ•°æ®ç®¡é“"""
        if self.dataloader is None:
            self.print_warning("æœªæä¾›æ•°æ®åŠ è½½å™¨ï¼Œè·³è¿‡æ•°æ®æ£€æŸ¥")
            return
            
        self.print_header("æ•°æ®ç®¡é“æ£€æŸ¥")
        
        dataset = self.dataloader.dataset
        self.print_success(f"æ•°æ®é›†ç±»å‹: {type(dataset).__name__}")
        self.print_success(f"æ•°æ®é›†å¤§å°: {len(dataset):,} æ ·æœ¬")
        self.print_success(f"Batchå¤§å°: {self.dataloader.batch_size}")
        self.print_success(f"Batchæ•°é‡: {len(self.dataloader)}")
        
        # æ£€æŸ¥ä¸€ä¸ªbatchçš„æ•°æ®
        try:
            sample_batch = next(iter(self.dataloader))
            
            if isinstance(sample_batch, (list, tuple)):
                inputs, targets = sample_batch
                self.print_success(f"è¾“å…¥å½¢çŠ¶: {inputs.shape}")
                self.print_success(f"æ ‡ç­¾å½¢çŠ¶: {targets.shape}")
                self.print_success(f"è¾“å…¥æ•°æ®ç±»å‹: {inputs.dtype}")
                
                # æ•°æ®èŒƒå›´æ£€æŸ¥
                if inputs.dtype in [torch.float32, torch.float64]:
                    self.print_success(f"è¾“å…¥æ•°æ®èŒƒå›´: [{inputs.min().item():.3f}, {inputs.max().item():.3f}]")
                    if inputs.min() < -10 or inputs.max() > 10:
                        self.print_warning("è¾“å…¥æ•°æ®èŒƒå›´è¾ƒå¤§ï¼Œè€ƒè™‘å½’ä¸€åŒ–")
                
                # NaN/Infæ£€æŸ¥
                if torch.isnan(inputs).any():
                    self.print_error("è¾“å…¥æ•°æ®åŒ…å«NaNå€¼!")
                if torch.isinf(inputs).any():
                    self.print_error("è¾“å…¥æ•°æ®åŒ…å«Infå€¼!")
                    
            else:
                self.print_success(f"æ•°æ®å½¢çŠ¶: {sample_batch.shape}")
                
        except Exception as e:
            self.print_error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    def check_model_architecture(self):
        """æ£€æŸ¥æ¨¡å‹æ¶æ„"""
        if self.model is None:
            self.print_warning("æœªæä¾›æ¨¡å‹ï¼Œè·³è¿‡æ¨¡å‹æ£€æŸ¥")
            return
            
        self.print_header("æ¨¡å‹æ¶æ„æ£€æŸ¥")
        
        self.print_success(f"æ¨¡å‹ç±»å‹: {type(self.model).__name__}")
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        non_trainable_params = total_params - trainable_params
        
        self.print_success(f"æ€»å‚æ•°é‡: {total_params:,}")
        self.print_success(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.print_success(f"ä¸å¯è®­ç»ƒå‚æ•°: {non_trainable_params:,}")
        
        if trainable_params == 0:
            self.print_error("æ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°!")
        
        # è®¾å¤‡æ£€æŸ¥
        device = next(self.model.parameters()).device
        self.print_success(f"æ¨¡å‹è®¾å¤‡: {device}")
        
        # å‰å‘ä¼ æ’­æµ‹è¯•
        if self.input_shape:
            try:
                self.model.eval()
                with torch.no_grad():
                    test_input = torch.randn(*self.input_shape).to(device)
                    output = self.model(test_input)
                    self.print_success(f"å‰å‘ä¼ æ’­æµ‹è¯•: {test_input.shape} -> {output.shape}")
                    
                    # è¾“å‡ºèŒƒå›´æ£€æŸ¥
                    if torch.isnan(output).any():
                        self.print_error("æ¨¡å‹è¾“å‡ºåŒ…å«NaNå€¼!")
                    if torch.isinf(output).any():
                        self.print_error("æ¨¡å‹è¾“å‡ºåŒ…å«Infå€¼!")
                        
            except Exception as e:
                self.print_error(f"å‰å‘ä¼ æ’­å¤±è´¥: {e}")
    
    def check_training_configuration(self):
        """æ£€æŸ¥è®­ç»ƒé…ç½®"""
        self.print_header("è®­ç»ƒé…ç½®æ£€æŸ¥")
        
        if self.optimizer:
            self.print_success(f"ä¼˜åŒ–å™¨: {type(self.optimizer).__name__}")
            lr = self.optimizer.param_groups[0]['lr']
            self.print_success(f"å­¦ä¹ ç‡: {lr}")
            
            if lr > 1.0:
                self.print_warning("å­¦ä¹ ç‡å¯èƒ½è¿‡é«˜")
            elif lr < 1e-6:
                self.print_warning("å­¦ä¹ ç‡å¯èƒ½è¿‡ä½")
        
        if self.criterion:
            self.print_success(f"æŸå¤±å‡½æ•°: {self.criterion.__class__.__name__}")
        
        if self.model:
            mode = "è®­ç»ƒ" if self.model.training else "è¯„ä¼°"
            self.print_success(f"æ¨¡å‹æ¨¡å¼: {mode}")
    
    def check_memory_estimation(self):
        """é¢„ä¼°å†…å­˜ä½¿ç”¨"""
        if not self.model or not self.dataloader or not self.input_shape:
            return
            
        self.print_header("å†…å­˜ä½¿ç”¨é¢„ä¼°")
        
        if torch.cuda.is_available():
            # æ¨¡å‹å‚æ•°å†…å­˜
            param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
            buffer_size = sum(b.numel() * b.element_size() for b in self.model.buffers())
            
            # æ¢¯åº¦å†…å­˜
            gradient_size = param_size
            
            # ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜ï¼ˆAdamçº¦ä¸ºå‚æ•°çš„2å€ï¼‰
            optimizer_factor = 2 if isinstance(self.optimizer, torch.optim.Adam) else 1
            optimizer_size = param_size * optimizer_factor
            
            # æ¿€æ´»å†…å­˜ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
            batch_size = self.dataloader.batch_size
            activation_size = batch_size * np.prod(self.input_shape) * 4  # float32
            
            total_memory = param_size + buffer_size + gradient_size + optimizer_size + activation_size
            total_gb = total_memory / 1e9
            
            self.print_success(f"é¢„ä¼°GPUå†…å­˜ä½¿ç”¨: {total_gb:.2f} GB")
            
            available_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            if total_gb > available_memory * 0.8:
                self.print_warning("é¢„ä¼°å†…å­˜ä½¿ç”¨è¶…è¿‡GPUæ˜¾å­˜çš„80%ï¼Œå¯èƒ½é‡åˆ°å†…å­˜ä¸è¶³é—®é¢˜")
            else:
                self.print_success("å†…å­˜é¢„ä¼°å®‰å…¨")
    
    def run_complete_check(self):
        """è¿è¡Œå®Œæ•´æ£€æŸ¥"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒå‰å®Œæ•´æ£€æŸ¥")
        print(f"ğŸ“… æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.check_system_environment()
        self.check_hardware_resources()
        self.check_data_pipeline()
        self.check_model_architecture()
        self.check_training_configuration()
        self.check_memory_estimation()
        
        self.print_header("æ£€æŸ¥æ€»ç»“")
        self.print_success("è®­ç»ƒå‰æ£€æŸ¥å®Œæˆ!")
        
        # æœ€ç»ˆå»ºè®®
        if not torch.cuda.is_available():
            self.print_warning("å»ºè®®: ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
        
        if self.model and next(self.model.parameters()).device.type == 'cpu' and torch.cuda.is_available():
            self.print_warning("å»ºè®®: å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU")


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # å‡è®¾ä½ æœ‰ä»¥ä¸‹ç»„ä»¶
    model = nn.Sequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 10)
    )
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    X = torch.randn(1000, 784)
    y = torch.randint(0, 10, (1000,))
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # è¿è¡Œæ£€æŸ¥
    checker = TrainingPreCheck(
        model=model,
        dataloader=dataloader,
        optimizer=optimizer,
        criterion=criterion,
        input_shape=(1, 784)  # è¾“å…¥å½¢çŠ¶
    )
    
    checker.run_complete_check()


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œç¤ºä¾‹
    example_usage()
    
    # æˆ–è€…ä½¿ç”¨ä½ è‡ªå·±çš„ç»„ä»¶
    # checker = TrainingPreCheck(
    #     model=your_model,
    #     dataloader=your_dataloader,
    #     optimizer=your_optimizer,
    #     criterion=your_criterion,
    #     input_shape=your_input_shape
    # )
    # checker.run_complete_check()